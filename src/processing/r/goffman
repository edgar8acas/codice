#! /usr/bin/Rscript
suppressPackageStartupMessages(library(tm))
suppressPackageStartupMessages(library(jsonlite))
suppressPackageStartupMessages(library(quanteda))
suppressPackageStartupMessages(library(dplyr))
suppressPackageStartupMessages(library(tibble))

#Reads and converts the input file
#TODO: pass filename as argument
input <- fromJSON(file("./io/input.json"))

#Create tmCorpus
tmCorpus <- Corpus( DataframeSource(input), readerControl=list(language="es"))

#PREPARACIÓN DEL TEXTO
tmCorpus <- tm_map(tmCorpus, removePunctuation)
tmCorpus <- tm_map(tmCorpus, stripWhitespace)
tmCorpus <- tm_map(tmCorpus, removeNumbers)
tmCorpus <- tm_map(tmCorpus, content_transformer(tolower))

#quanteda (quanteda)
qCorpus<-corpus(tmCorpus)

#tokenize each text (quanteda)
#No se usa más adelante la variable qtok
qtok<-tokens(qCorpus)

#create a document-feature matrix (dfm) from a corpus
qdfm<-dfm(qCorpus)

#Weight a dfm by term frequency-inverse document frequency (tf-idf), with full control over options.
#No se usa más adelante
qtf_idf<-dfm_tfidf(qdfm)

qtf_idf_matrix <- as.matrix(qtf_idf)

#transpose the matrix generated by qdfm
qdfm_matrix <- as.matrix(qdfm)
transp_qdfm<-t(qdfm_matrix)
#transp_qdfm<-t(qtf_idf_matrix)
transp_qdfm_df <- as.data.frame(transp_qdfm)


#obtain the sum of each word in the whole collection
columns <- ncol(transp_qdfm_df) + 1
collection_freq <- 
        transp_qdfm_df %>%
        rownames_to_column('words') %>%
        mutate(sum = rowSums(.[2:columns])) %>%
        column_to_rownames('words')

#total words
col_total_words <- summarise_at(collection_freq, c('sum'), sum)[1,1]
#col_total_words <- 1172

#different words
diff_words <- rownames(transp_qdfm)

#get words with frequency 1
#No se usa más adelante
#freq_1_words<-collection_freq[collection_freq==1]

#No se usa más adelante
#quantity of words with frequency 1
#H<-sum(freq_1_words)

#No se usa más adelante
#CALCULO DEL PUNTO DE TRANSICIÓN, 
#SOLO SE REQUIERE DETERMINAR EL NÚMERO DE PALABRAS CUYA FRECUENCIA SEA 1
#tg<-(-1+(sqrt((1+8*(H)))/2))
  
stopwords <- stopwords("es")

#Cálculo de Luhn (total en librería)
luhn_lib<- col_total_words / (length(diff_words)-length(stopwords))

#Cálculo de Luhn (solo existentes en la colección)
col_stopwords <- diff_words[diff_words %in% stopwords]
luhn_col<-col_total_words / (length(diff_words)-length(col_stopwords))
#e <- stopwords[stopwords %in% diff_words]
#all(unique(sort(d)) == unique(sort(e)))

col_freq_luhn_lib <- 
  collection_freq %>%
  rownames_to_column('words') %>%
  filter(sum > luhn_lib) %>%
  column_to_rownames('words')

col_freq_luhn <- 
  collection_freq %>%
  rownames_to_column('words') %>%
  filter(sum > luhn_col)


# MATRIZ FINAL, a partir de esta matriz, aquellas palabras con frecuencia mayor a cero
# en cada columna, son las que se exportarán como palabras esenciales por texto
col_freq_luhn <- 
  col_freq_luhn %>%
  filter(!words %in% stopwords)


## Preparo una lista para exportar la info final a json
output <- list()
for(doc_id in  input$doc_id) {
  print(class(doc_id))
  output[[doc_id]] <-
    col_freq_luhn %>%
    filter(!! rlang::sym(doc_id) > 0) %>%
    pull(words)
}

# for(doc_id in  input$doc_id) {
#   print(doc_id)
#   output[[doc_id]] <-
#     col_freq_luhn %>%
#     filter(!! rlang::sym(doc_id) > 0) %>%
#     pull(words)
# }

# list <- list(x = 1:5, y = c('a', 'b', list(c(6, 7, 8))))
# str(list)
# 
# df <- data.frame()
# df
# toJSON(df)
# toJSON(list)
# 
# df <- tibble_row(textId = 23, words = list(list('a', 'b')), lecturability = 50)
# df
# str(df)
# str(flatten(df))
# str(df)

# df[["words"]]
# toJSON(df, flatten = TRUE, simplifyVector = TRUE)
# str(c('a', 'b'))
con_out <- file("./io/output.json", open = "wb")
output
converted <- toJSON(output)
write(converted, con_out)

# x <- data.frame(driver = c("Bowser", "Peach"), occupation = c("Koopa", "Princess"))
# x$vehicle <- data.frame(model = c("Piranha Prowler", "Royal Racer"))
# x$vehicle$stats <- data.frame(speed = c(55, 34), weight = c(67, 24), drift = c(35, 32))
# str(x)
# str(flatten(x))
# str(flatten(x, recursive = FALSE))